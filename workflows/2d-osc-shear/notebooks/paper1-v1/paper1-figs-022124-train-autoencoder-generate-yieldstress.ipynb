{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import gsd.hoomd\n",
    "import schmeud._schmeud as schmeud_rs\n",
    "from schmeud._schmeud import statics\n",
    "from schmeud import ml\n",
    "from tqdm import tqdm\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "import signac\n",
    "import freud\n",
    "from numba import njit\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import hoomd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monk import workflow, utils, prep as prep_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/home/ian/Projects/work/monk/workflows/2d-osc-shear/config.yaml'),\n",
       " {'root': '/media/ian/Data2/monk/2d-osc-shear',\n",
       "  'origin': '/media/ian/Data2/monk/2d-esl'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent = pathlib.Path(os.getcwd()).parent / \"config.yaml\"\n",
    "config = workflow.get_config(parent.as_posix())\n",
    "parent, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avail_seed': 18, 'dt': 0.005, 'step_unit': 200, 'equil_time': 100, 'min_periods': 20, 'dumps': 40, 'period_times': [30.0, 100.0, 300.0, 1000.0], 'max_shears': [0.01, 0.02, 0.03, 0.05, 0.08, 0.12, 0.04, 0.06, 0.07], '_status': {}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project: signac.Project = signac.get_project(root=config['root'])\n",
    "project.doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = None\n",
    "pipe0 = None\n",
    "pipe1 = None\n",
    "with open(\"svc.pkl\", \"rb\") as f:\n",
    "    pipe = pickle.load(f)\n",
    "\n",
    "with open(\"svc_type0.pkl\", \"rb\") as f:\n",
    "    pipe0 = pickle.load(f)\n",
    "\n",
    "with open(\"svc_type1.pkl\", \"rb\") as f:\n",
    "    pipe1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True, eq=True)\n",
    "class Statepoint:\n",
    "    max_shear: float\n",
    "    period: float\n",
    "    temp: float\n",
    "    prep: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6976a7c10ada4159010403f8027f3ea5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecce68c50e28a33684826f28780bf6e9\n",
      "cfa1e4c0007fd65d020f809715a6f0b4\n",
      "1083a2a09eec2c6cf215ebc7a9706cac\n",
      "634e14bc75836df75158fb15f379d8bb\n",
      "9158e73b2e4c9fd1a565e1e5d1bc3af5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for job in project.find_jobs({\"prep\": \"HTL\"}):\n",
    "    print(job)\n",
    "    prep = job.sp[\"prep\"]\n",
    "    # if prep != \"ESL\":\n",
    "    #     continue\n",
    "    # print(prep)\n",
    "    \n",
    "    experiments = sorted(glob.glob(job.fn(\"longer_experiments/*/*/traj-fire_period-*.gsd\")))\n",
    "    if len(experiments) == 0:\n",
    "        continue\n",
    "    # inter_computer = InteractionComputer(experiments[0], job.sp[\"pot\"], frame=0)\n",
    "    for exper in experiments:\n",
    "        max_shear = utils.extract_between(exper, \"max-shear-\", \"/\")\n",
    "        period = utils.extract_between(exper, \"period-\", \".gsd\")\n",
    "        temp = utils.extract_between(exper, \"temp-\", \"/\")\n",
    "        df_path = f\"longer_experiments/max-shear-{max_shear}/temp-{temp}/auto-encoder-dataset-yieldstress_period-{period}.parquet\"\n",
    "        sp = Statepoint(max_shear=float(max_shear), period=float(period), temp=float(temp), prep=prep)\n",
    "        \n",
    "        if float(period) != 1000.0 or float(temp) != 0.019836 or float(max_shear) != 0.04:\n",
    "            continue\n",
    "\n",
    "        # if job.isfile(df_path):\n",
    "        #     # dataset = pl.read_parquet(job.fn(df_path))\n",
    "        #     # output[sp].append(dataset)\n",
    "        #     continue\n",
    "\n",
    "        traj = gsd.hoomd.open(exper)\n",
    "\n",
    "        oframes = []\n",
    "        ostrain = []\n",
    "        oid = []\n",
    "        osoft = []\n",
    "        osfs = []\n",
    "        # od2min_irr_10 = []\n",
    "        # od2min_rev_10 = []\n",
    "        # od2min_irr_20 = []\n",
    "        # od2min_rev_20 = []\n",
    "\n",
    "        frame = len(traj) - 1\n",
    "\n",
    "        # start = 1\n",
    "        # end = 101\n",
    "        # cycle_start_idx = lambda i: -1 + i*40\n",
    "        # # for frame in tqdm(range(cycle_start_idx(start), cycle_start_idx(end),\n",
    "        # # 40)):\n",
    "        # for frame in range(cycle_start_idx(start), cycle_start_idx(end), 40*20):\n",
    "\n",
    "        # inter_computer.set_frame(frame)\n",
    "        # virials = inter_computer.get_virials()\n",
    "        # xx = virials[:, 0]\n",
    "        # yy = virials[:, 3]\n",
    "        # xy = virials[:, 1]\n",
    "\n",
    "        # inter_computer.set_frame(frame + 1)\n",
    "        # virials = inter_computer.get_virials()\n",
    "        # xx1 = virials[:, 0]\n",
    "        # yy1 = virials[:, 3]\n",
    "        # xy1 = virials[:, 1]\n",
    "\n",
    "        # inter_computer.set_frame(frame + 2)\n",
    "        # virials = inter_computer.get_virials()\n",
    "        # xx2 = virials[:, 0]\n",
    "        # yy2 = virials[:, 3]\n",
    "        # xy2 = virials[:, 1]\n",
    "\n",
    "        # xxs = np.stack((xx, xx1, xx2), axis=-1)\n",
    "        # yys = np.stack((yy, yy1, yy2), axis=-1)\n",
    "        # xys = np.stack((xy, xy1, xy2), axis=-1)\n",
    "\n",
    "        snap = traj[frame]\n",
    "\n",
    "        typeids = snap.particles.typeid\n",
    "        softb = np.zeros_like(typeids, dtype=np.float32)\n",
    "\n",
    "        query_indices0 = np.arange(snap.particles.N)[typeids == 0]\n",
    "        sfs0 = ml.compute_structure_functions_snap(snap, query_indices0)\n",
    "        soft0 = pipe0.decision_function(sfs0)\n",
    "\n",
    "        query_indices1 = np.arange(snap.particles.N)[typeids == 1]\n",
    "        sfs1 = ml.compute_structure_functions_snap(snap, query_indices1)\n",
    "        soft1 = pipe1.decision_function(sfs1)\n",
    "\n",
    "        softb[typeids == 0] = soft0\n",
    "        softb[typeids == 1] = soft1\n",
    "\n",
    "        # softness.append(softb)\n",
    "\n",
    "        all_sfs = np.zeros((snap.particles.N, sfs1.shape[1]), dtype=np.float32)\n",
    "\n",
    "        all_sfs[typeids == 0] = sfs0\n",
    "        all_sfs[typeids == 1] = sfs1\n",
    "\n",
    "        # get softness within r_max\n",
    "        # nlist_query = freud.locality.LinkCell.from_system(snap)\n",
    "        # nlist = nlist_query.query(snap.particles.position, {'exclude_ii': True, \"r_max\": 40.0}).toNeighborList()\n",
    "\n",
    "        # neighbor_softs = softb[nlist.point_indices]\n",
    "        # local_soft = get_local_softness(softb, nlist.segments, nlist.neighbor_counts, nlist.point_indices, nlist.distances, 40.0, 40)\n",
    "        \n",
    "\n",
    "        frames = np.ones_like(typeids, dtype=np.int32) * frame\n",
    "\n",
    "        strains = np.ones_like(typeids, dtype=np.float32) * float(max_shear)\n",
    "\n",
    "        # snap_high = traj[10 + frame]\n",
    "        # snap_later = traj[40 + frame]\n",
    "        # snap_low = traj[30 + frame]\n",
    "\n",
    "        # box = snap.configuration.box[:]\n",
    "        # box_high = snap_high.configuration.box[:]\n",
    "        # box_later = snap_later.configuration.box[:]\n",
    "        # box_low = snap_low.configuration.box[:]\n",
    "\n",
    "        # nlist_query = freud.locality.LinkCell.from_system(snap)\n",
    "        # nlist = nlist_query.query(snap.particles.position, {'num_neighbors': 10}).toNeighborList()\n",
    "\n",
    "        # d2min_irr = schmeud_rs.dynamics.d2min_frame(snap.particles.position[:, :2], snap_later.particles.position[:, :2], nlist.query_point_indices, nlist.point_indices, (box, box_later))\n",
    "        # d2min_rev = schmeud_rs.dynamics.d2min_frame(snap_high.particles.position[:, :2], snap_low.particles.position[:, :2], nlist.query_point_indices, nlist.point_indices, (box_high, box_low))\n",
    "        # # d2min_c = np.sqrt(np.square(d2min_rev) + np.square(d2min_irr)) * np.sign(d2min_rev - d2min_irr)\n",
    "\n",
    "        # nlist_query = freud.locality.LinkCell.from_system(snap)\n",
    "        # nlist = nlist_query.query(snap.particles.position, {'num_neighbors': 20}).toNeighborList()\n",
    "\n",
    "        # d2min_irr_20 = schmeud_rs.dynamics.d2min_frame(snap.particles.position[:, :2], snap_later.particles.position[:, :2], nlist.query_point_indices, nlist.point_indices, (box, box_later))\n",
    "        # d2min_rev_20 = schmeud_rs.dynamics.d2min_frame(snap_high.particles.position[:, :2], snap_low.particles.position[:, :2], nlist.query_point_indices, nlist.point_indices, (box_high, box_low))\n",
    "        # d2min_c = np.sqrt(np.square(d2min_rev) + np.square(d2min_irr)) *\n",
    "        # np.sign(d2min_rev - d2min_irr)\n",
    "        \n",
    "        oframes.append(frames)\n",
    "        ostrain.append(strains)\n",
    "        oid.append(typeids)\n",
    "        osoft.append(softb)\n",
    "        osfs.append(all_sfs)\n",
    "        # od2min_irr_10.append(d2min_irr)\n",
    "        # od2min_rev_10.append(d2min_rev)\n",
    "        # od2min_irr_20.append(d2min_irr_20)\n",
    "        # od2min_rev_20.append(d2min_rev_20)\n",
    "\n",
    "        frames = np.concatenate(oframes)\n",
    "        strains = np.concatenate(ostrain)\n",
    "        typeids = np.concatenate(oid)\n",
    "        softb = np.concatenate(osoft)\n",
    "        all_sfs = np.concatenate(osfs)\n",
    "        # d2min_irr = np.concatenate(od2min_irr_10)\n",
    "        # d2min_rev = np.concatenate(od2min_rev_10)\n",
    "        # d2min_irr_20 = np.concatenate(od2min_irr_20)\n",
    "        # d2min_rev_20 = np.concatenate(od2min_rev_20)\n",
    "\n",
    "        dataset = pl.DataFrame({\"frame\": frames, \"strain\": strains, \"id\": np.array(typeids), \"soft\": np.array(softb), \"sfs\": all_sfs})\n",
    "    # dataset = dataset.explode([\"id\", \"soft\"]).reset_index(drop=True)\n",
    "        dataset.write_parquet(job.fn(df_path), use_pyarrow=True)\n",
    "    #     break\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
